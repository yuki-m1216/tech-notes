# リソース監視

クラウド環境におけるリソース監視の考え方と実践方法を学びます。

!!! note "閾値について"
    このドキュメントに記載されている閾値はあくまで一例です。実際のシステムでは、トラフィックパターン、アプリケーション特性、インフラ構成などに応じて適切に調整してください。

## リソース監視の目的

### 1. ユーザー影響の予兆検知

リソース逼迫によるサービス停止を事前に防ぐことが最優先です。

**監視の観点:**
- エンドユーザーに影響が出る前に検知
- 時間的猶予を持って対応できる閾値設定
- 予兆段階での警告

### 2. キャパシティプランニング

長期的なリソース使用傾向を把握し、適切な規模にシステムを調整します。

**監視の観点:**
- 週次/月次での傾向分析
- 成長に合わせたスケール計画
- コスト最適化の判断材料

## 主要なリソースメトリクス

### 1. コンピュートリソース

#### EC2 / ECS

**CPU使用率**

```
監視項目: CPUUtilization
アラート設定例:
- CPU > 90%（5分間継続） → Critical（Auto Scaling失敗時）
- CPU > 80%（15分間継続） → Warning（週次レビュー時確認）
- CPU使用率の推移 → Info（ダッシュボード表示）

考え方:
- Auto Scalingが正常に機能していれば、80%は想定内
- 90%超えでスケールアウトしていない場合は問題
- 閾値はシステム特性に応じて調整が必要
```

**メモリ使用率**

```
監視項目: MemoryUtilization（CloudWatch Agentで取得）
アラート設定例:
- Memory > 90%（5分間継続） → Critical
- Memory > 80%（15分間継続） → Warning（週次レビュー時確認）

考え方:
- メモリ不足はスワップやOOM Killerにつながる
- Auto Scalingでスケールアウトすれば、処理が分散されメモリ負荷も分散される
- 閾値はアプリケーション特性により調整
```

#### Lambda

**実行時間**

```
監視項目: Duration
アラート設定例:
- タイムアウト発生 → Critical（ユーザー影響あり）
- 実行時間が平均の2倍 → Warning（週次レビュー時確認）
- 実行時間の推移 → Info（ダッシュボード表示）

考え方:
- タイムアウトは直接ユーザー影響がある
- 実行時間の増加傾向はコード改善やリソース調整の検討材料
```

**同時実行数**

```
監視項目: ConcurrentExecutions
アラート設定例:
- 同時実行数 > アカウント制限の80% → Warning
- スロットリング発生 → Critical（ユーザー影響あり）

考え方:
- 制限に達するとスロットリングが発生し、ユーザー影響がある
```

### 2. データベースリソース

#### RDS / Aurora

**接続数**

```
監視項目: DatabaseConnections
アラート設定例:
- 接続数 > 最大接続数の95% → Critical
- 接続数 > 最大接続数の80% → Warning（週次レビュー時確認）
- 接続数の推移 → Info（ダッシュボード表示）

考え方:
- 95%を超えると新規接続が拒否される可能性が高い
- 80%超えは適切なコネクションプーリング設定の見直しを検討
- 閾値はアプリケーションの接続パターンにより調整
```

**CPU使用率**

```
監視項目: CPUUtilization
アラート設定例:
- CPU > 90%（5分間継続） → Critical
- CPU > 80%（15分間継続） → Warning（週次レビュー時確認）

考え方:
- Aurora の場合、Read Replicaのオートスケーリングで対応可能
- Aurora Serverless v2 の場合、ACU（Aurora Capacity Units）が自動調整される
- RDS（非Aurora）の場合、スケールアップ（インスタンスサイズ変更）が必要
- スロークエリの可能性、インデックス追加やクエリ最適化を検討
```

**ストレージ容量**

```
監視項目: FreeStorageSpace
アラート設定例:
- 空き容量 < 10% → Critical
- 空き容量 < 20% → Warning（週次レビュー時確認）
- 容量使用率の推移 → Info（ダッシュボード表示）

考え方:
- ストレージ枯渇はデータベースが停止する
- ストレージ自動スケーリングを有効にしている場合は自動拡張される
- 閾値はデータ増加速度により調整
```

#### DynamoDB

**キャパシティ使用率**

```
監視項目: ConsumedReadCapacityUnits / ConsumedWriteCapacityUnits
アラート設定例:
- スロットリング発生 → Critical（ユーザー影響あり）
- キャパシティ使用率 > 80% → Warning（週次レビュー時確認）

考え方:
- スロットリングは直接ユーザー影響がある
- On-Demandモードではスロットリングは稀だが、コストに注意
- Provisioned モードの場合、Auto Scalingの設定を確認
```

### 3. ネットワークリソース

#### ALB

**TargetHealth**

```
監視項目: UnhealthyHostCount / HealthyHostCount
アラート設定例:
- 全ターゲットUnhealthy → Critical
- 一部ターゲットUnhealthy → Warning（システム特性による）

考え方:
- 全ターゲットダウンはサービス停止
- 一部ダウンは冗長性設計により影響が異なる
```

**レスポンスタイム**

```
監視項目: TargetResponseTime
アラート設定例:
- レスポンスタイム > SLO → Critical
- レスポンスタイム > 通常の2倍 → Warning
- レスポンスタイムの推移 → Info（ダッシュボード表示）

考え方:
- SLO違反はユーザー影響がある
- 継続的な遅延傾向はボトルネックの調査が必要
- 閾値はSLOに基づいて設定
```

### 4. ストレージリソース

#### S3

**バケット容量**

```
監視項目: BucketSizeBytes
アラート設定例:
- 容量の推移 → Info（ダッシュボード表示）

考え方:
- S3は実質無制限のため、容量枯渇の心配はない
- コスト最適化の観点で監視（ライフサイクルポリシーの検討）
```

#### EBS

**ディスク使用率**

```
監視項目: disk_used_percent（CloudWatch Agentで取得）
アラート設定例:
- 使用率 > 90% → Critical
- 使用率 > 80% → Warning（週次レビュー時確認）

考え方:
- ディスク枯渇はアプリケーションの停止につながる
- ログファイルの肥大化などに注意
```

## Auto Scaling環境でのリソース監視

### 基本的な考え方

Auto Scalingが正常に機能していれば、リソース使用率の上昇は一時的で問題ありません。

**監視の重点:**

1. **Auto Scaling自体の動作監視**
   - スケーリングアクティビティの成功/失敗
   - Desired Capacity と実際のインスタンス数の一致

2. **スケール失敗時の検知**
   - リソース使用率が高いままスケールアウトしない
   - インスタンス起動の失敗

### アラート設計例

```
✅ 正常なケース:
- CPU 80% → Auto Scaling発動 → インスタンス増加 → CPU低下
  → Warning（週次レビューで確認、必要ならスケーリングポリシー調整）

❌ 異常なケース:
- CPU 90% → Auto Scaling発動せず → Critical（即時対応）
- CPU 90% → Auto Scaling失敗 → Critical（即時対応）
```

## リソース監視のベストプラクティス

### 1. エンドユーザー影響を基準にする

リソース使用率自体は問題ではなく、**エンドユーザーへの影響**を重視します。

```
✅ CPU 80%だが、レスポンスタイムは正常 → Warning（週次レビュー）
✅ CPU 50%だが、レスポンスタイムが遅い → Critical（即時調査）
```

### 2. 適切な閾値を設定する

過去のデータを分析し、誤検知を防ぐ閾値を設定します。

```
✅ 過去30日間のデータを分析
✅ 通常時の最大値 + マージンで閾値を設定
✅ 閾値は定期的に見直す
✅ システム特性に応じて柔軟に調整
```

### 3. 評価期間を適切に設定する

一時的なスパイクで誤検知しないよう、適切な評価期間を設定します。

```
✅ CPU > 90% を 5分間継続 → アラート発火
❌ CPU > 90% を 1回 → 誤検知の可能性
```

### 4. ダッシュボードで全体を可視化する

各リソースの使用状況を一つのダッシュボードで確認できるようにします。

**ダッシュボードに含める情報:**
- CPU、メモリ、ディスク使用率の推移
- ネットワークトラフィックの推移
- データベース接続数の推移
- エラー率、レスポンスタイムの推移

### 5. 週次/月次レビューで傾向を把握する

定期的にダッシュボードを確認し、長期的な傾向を分析します。

**レビューの観点:**
- リソース使用率の増加傾向
- スケーリングポリシーの妥当性
- コスト最適化の機会
- キャパシティプランニングの必要性

## よくある失敗パターン

### 1. 全てのリソースを同じ優先度で監視

**問題**: 重要度の低いアラートに時間を取られる

**対策**: エンドユーザー影響を基準に優先度を設定

```
❌ S3バケット容量増加 → Critical
✅ S3バケット容量増加 → Info（ダッシュボード確認）
```

### 2. Auto Scaling環境で固定閾値を使用

**問題**: スケールアウト時に誤検知が多発

**対策**: Auto Scalingの動作を考慮した閾値設定

```
❌ インスタンス数 < 2台 → Critical
✅ Desired Capacity != 実際のインスタンス数 → Critical
```

### 3. 閾値が低すぎる

**問題**: 誤検知が多発し、アラートが無視される

**対策**: 過去のデータを分析し、適切な閾値を設定

### 4. 評価期間が短すぎる

**問題**: 一時的なスパイクで不要なアラートが発生

**対策**: 5分間継続など、適切な評価期間を設定

### 5. システム特性を考慮しない一律の閾値

**問題**: 異なる特性のシステムに同じ閾値を適用

**対策**: 各システムのトラフィックパターンやアプリケーション特性に応じて調整

## まとめ

- リソース監視の目的はエンドユーザー影響の予兆検知とキャパシティプランニング
- エンドユーザー影響を基準にアラートレベルを設定する
- Auto Scalingが正常なら、リソース使用率の上昇は問題ない
- 適切な閾値と評価期間を設定し、誤検知を防ぐ
- ダッシュボードで全体を可視化し、週次/月次レビューで傾向を把握する
- リソース使用率自体ではなく、ユーザー体験への影響を重視する
- **閾値はあくまで一例であり、システム特性に応じて調整が必要**
